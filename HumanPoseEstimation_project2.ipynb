{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HumanPoseEstimation_project2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNiRjas+TibPxH7ZIHYV9ly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timgluz/colab_notebooks/blob/master/HumanPoseEstimation_project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNJs6MaUqQWE",
        "colab_type": "text"
      },
      "source": [
        "# Loading data\n",
        "\n",
        "Tutorial and hints about the dataset: \n",
        "\n",
        "https://www.tensorflow.org/datasets/catalog/svhn_cropped\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3sExRfVp49M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, info = tfds.load(\n",
        "    name = \"svhn_cropped\",\n",
        "    with_info= True,\n",
        "    batch_size = 32,\n",
        "    as_supervised = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I2WCR6W1NP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing data\n",
        "print(f\"Training size: {info.splits['train'].num_examples}\")\n",
        "print(f\"Test size: {info.splits['test'].num_examples}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYEVb5Jo4bDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svhn_train = dataset['train'].prefetch(1)\n",
        "svhn_test = dataset['test'].prefetch(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRyuItF_KVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = dataset['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwrbSoVbtWlk",
        "colab_type": "text"
      },
      "source": [
        "# Building a model\n",
        "\n",
        "Let’s now implement the architecture with 3 convolutional layers. Check the dimensions of the feature maps after each layer.\n",
        "\n",
        "**Notice** how the dimensions change when you change padding or stride. The configuration below should give you an accuracy of about 88% on the SVHN dataset. Note: Make sure you use the “2d”-versions of the units.\n",
        "\n",
        "```\n",
        " 1. convolution, kernel_size=5, channels=6, stride=1, padding=2\n",
        " 2. batch-normalization\n",
        " 3. ReLU\n",
        " 4. Max-pool, kernel_size=2, stride=2\n",
        "\n",
        " 5. convolution, kernel_size=3, channels=12, stride=1, padding=1\n",
        " 6. batch-normalization\n",
        " 7. ReLU\n",
        " 8. Max-pool, kernel_size=2, stride=2\n",
        "\n",
        " 9. convolution, kernel_size=3, channels=24, stride=1, padding=1\n",
        " 10. batch-normalization\n",
        " 11. ReLU\n",
        " 12. Max-pool, kernel_size=2, stride=2\n",
        "\n",
        " 13. fully connected layer, output_size=10\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jiLlNistV7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, ReLU, Dense\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "model  = tf.keras.models.Sequential([\n",
        "  Rescaling(1.0/255, input_shape = [32, 32, 3]),                            \n",
        "  Conv2D(16, kernel_size = 5, strides = (1,1),\n",
        "         input_shape= [32, 32, 3], padding = \"valid\"),\n",
        "  BatchNormalization(),\n",
        "  ReLU(),\n",
        "\n",
        "  # 2nd layer\n",
        "  Conv2D(32, kernel_size = 3, strides = (1, 1), padding = \"valid\"),\n",
        "  BatchNormalization(),\n",
        "  ReLU(),\n",
        "  MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n",
        "\n",
        "  # 3rd layer\n",
        "  Conv2D(64, kernel_size = 3, strides = (1, 1), padding = \"valid\"),\n",
        "  BatchNormalization(),\n",
        "  ReLU(),\n",
        "  MaxPool2D(pool_size = (2, 2), strides = (2, 2)),\n",
        "\n",
        "  # final layer\n",
        "  Dense(10)\n",
        "])\n",
        "\n",
        "model.build()\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16u6VSdEA7qc",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "model.compile(\n",
        "  loss = \"categorical_crossentropy\",\n",
        "  optimizer = tf.keras.optimizers.Adam(lr = 0.001),\n",
        "  metrics = [\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(svhn_train, epochs = 5, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}