{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyCourse_chapter1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHhv4fvMxiTPU1IPsVltPk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timgluz/colab_notebooks/blob/master/SpacyCourse_chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvBMOXrgm06c",
        "colab_type": "text"
      },
      "source": [
        "  # Chapter.1 - Introduction to SpaCy\n",
        "\n",
        "  SpaCy is a popular library for advanced NLP in Python.\n",
        "\n",
        "source: https://course.spacy.io/en/chapter1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWydt5PVmbhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install spacy\n",
        "\n",
        "%pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbq-D9r0nLw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the english language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# create the nlp object\n",
        "# contains the procesing pipeline,\n",
        "# includes language-specific rules for tokenization\n",
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txTHQEdmnwuk",
        "colab_type": "text"
      },
      "source": [
        "When you process a text with the `nlp` object, spaCy creates a `Doc` object – short for \"document\". The `Doc` lets you access information about the text in a structured way, and no information is lost.\n",
        "\n",
        "The `Doc` behaves like a normal Python __sequence__ by the way and lets you iterate over its tokens, or get a token by its index. But more on that later!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnJOGJAToBHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Doc object from a string\n",
        "doc = nlp(\"Hello world!\")\n",
        "\n",
        "# iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "  print(token.text)\n",
        "\n",
        "print(\"\\n-----\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61jH-yXKo4s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# index into the Doc to get a single Token\n",
        "token = doc[1]\n",
        "\n",
        "# get the token text via .text attribute\n",
        "print(token.text)\n",
        "\n",
        "# a slice from the Doc is a Span object\n",
        "span = doc[1:3]\n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcKy1V1vpVaU",
        "colab_type": "text"
      },
      "source": [
        "### lexical attributes\n",
        "\n",
        "they refer to the entry in the vocabulary and don't depend on the token's context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMEg0aIvpeZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"It costs $5.\")\n",
        "\n",
        "# the i is the index of the token within the parent document.\n",
        "print(\"Index:   \", [token.i for token in doc])\n",
        "\n",
        "# text returns the token text\n",
        "print(\"Text:    \", [token.text for token in doc])\n",
        "\n",
        "# is_alpha indicates whether the token consists of alphabetic characters\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "\n",
        "# is_alpha indicates whether the token includes punctuation\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "\n",
        "# like_num whether it resembles a number\n",
        "print(\"like_num:\", [token.like_num for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cSM7nQqrn3w",
        "colab_type": "text"
      },
      "source": [
        "# Statistical models\n",
        "\n",
        "Statistical models enable spaCy to make predictions in context. This usually includes part-of speech tags, syntactic dependencies and named entities.\n",
        "\n",
        "Models are trained on large datasets of labeled example texts.\n",
        "\n",
        "Meaning of POS tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASo83Gxt-F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download model packages\n",
        "\n",
        "#%%python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# load a model package by name\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# process a text\n",
        "doc = nlp(\"She ate the pizza\")\n",
        "\n",
        "# print out each token and the predicted part-of-speec tag\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwFLFiH-vktA",
        "colab_type": "text"
      },
      "source": [
        "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
        "\n",
        "The `.dep_` attribute returns the predicted dependency label.\n",
        "\n",
        "The `.head` attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqaPD6AFvrr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_, token.head.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jTu8_QAwC1I",
        "colab_type": "text"
      },
      "source": [
        "#### Predicting name entities\n",
        "\n",
        "**Named entities** are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
        "\n",
        "The `doc.ents` property lets you access the named entities predicted by the model.\n",
        "\n",
        "It returns an iterator of **Span** objects, so we can print the entity text and the entity label using the `.label_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-f_NqVvwUaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and its label\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIvbbe1Iwgfk",
        "colab_type": "text"
      },
      "source": [
        "**A quick tip:** To get definitions for the most common tags and labels, you can use the spacy.explain helper function.\n",
        "\n",
        "For example, \"GPE\" for *geopolitical entity* isn't exactly intuitive – but `spacy.explain` can tell you that it refers to countries, cities and states.\n",
        "\n",
        "The same works for part-of-speech tags and dependency labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQfZVtXZw1UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(spacy.explain(\"GPE\"))\n",
        "# 'Countries, cities, states'\n",
        "\n",
        "print(spacy.explain(\"NNP\"))\n",
        "# 'noun, proper singular'\n",
        "\n",
        "print(spacy.explain(\"dobj\"))\n",
        "# 'direct object'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtO88Cjj1GK4",
        "colab_type": "text"
      },
      "source": [
        "# Rule-based matching\n",
        "\n",
        "spaCy's matcher, which lets you write rules to find words and phrases in text.\n",
        "\n",
        "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
        "\n",
        "It's also more flexible: you can search for texts but also other lexical attributes.\n",
        "\n",
        "You can even write rules that use the model's predictions.\n",
        "\n",
        "**Match patterns** are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6S-ie3d1lsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
        "\n",
        "# Process some text\n",
        "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VikWSmD2MdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matching lexical attributes\n",
        "pattern = [\n",
        "    {\"IS_DIGIT\": True},\n",
        "    {\"LOWER\": \"fifa\"},\n",
        "    {\"LOWER\": \"world\"},\n",
        "    {\"LOWER\": \"cup\"},\n",
        "    {\"IS_PUNCT\": True}\n",
        "]\n",
        "\n",
        "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_MbKWNK28Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matching other token attributes\n",
        "\n",
        "pattern = [\n",
        "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct1aGx1O3KIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using operators and quantifiers\n",
        "pattern = [\n",
        "    {\"LEMMA\": \"buy\"},\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}