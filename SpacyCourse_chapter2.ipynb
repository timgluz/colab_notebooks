{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyCourse_chapter2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS4Cmdb9VGurNMdlbhmYXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timgluz/colab_notebooks/blob/master/SpacyCourse_chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHYL8XeK6qFA",
        "colab_type": "text"
      },
      "source": [
        "# Data structures: Vocab, Lexemes, and StringStore\n",
        "\n",
        "source: https://course.spacy.io/en/chapter2\n",
        "\n",
        "In this lesson, we'll take a look at the shared vocabulary and how spaCy deals with strings.\n",
        "\n",
        "spaCy stores all shared data in a vocabulary, the **Vocab**.\n",
        "\n",
        "This includes words, but also the labels schemes for tags and entities.\n",
        "\n",
        "Strings are only stored once in the `StringStore` via `nlp.vocab.strings`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Y065J4A1V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install spacy\n",
        "\n",
        "%pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mty76M3y5ia0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download medium size model package\n",
        "%%python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cxI_6kWA_44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# import the english language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# create the nlp object\n",
        "# contains the procesing pipeline,\n",
        "# includes language-specific rules for tokenization\n",
        "nlp = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwWZCE226T1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look up the string and hash in nlp.vocab.string\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
        "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3_YqId0BSOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A Doc object also exposes its vocab and strings.\n",
        "\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wrc3YpiBlXU",
        "colab_type": "text"
      },
      "source": [
        "### Lexemes\n",
        "\n",
        "Lexemes are context-independent entries in the vocabulary.\n",
        "\n",
        "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
        "\n",
        "Lexemes expose attributes, just like tokens.\n",
        "\n",
        "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.\n",
        "\n",
        "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpStyoJ9Bv3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "lexeme = nlp.vocab[\"coffee\"]\n",
        "\n",
        "# Print the lexical attributes\n",
        "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyprcboIHePo",
        "colab_type": "text"
      },
      "source": [
        "### The Doc object\n",
        "\n",
        "The `Doc` is one of the central data structures in spaCy. It's created automatically when you process a text with the `nlp` object. But you can also instantiate the class manually.\n",
        "\n",
        "After creating the `nlp` object, we can import the `Doc` class from `spacy.tokens`.\n",
        "\n",
        "\n",
        "The spaces are a list of boolean values indicating whether the word is followed by a space. Every token includes that information â€“ even the last one!\n",
        "\n",
        "The Doc class takes three arguments: the shared vocab, the words and the spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvgfbX7SIAFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an nlp object\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls1QYvj0LhWc",
        "colab_type": "text"
      },
      "source": [
        "### The Span object\n",
        "\n",
        "A Span is a slice of a doc consisting of one or more tokens. The Span takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sx-lDNeLpW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "# Create a span manually\n",
        "span = Span(doc, 0, 2)\n",
        "\n",
        "# Create a span with a label\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
        "\n",
        "# Add span to the doc.ents\n",
        "doc.ents = [span_with_label]\n",
        "print(span_with_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdehX4EANrJM",
        "colab_type": "text"
      },
      "source": [
        "# Word vectors and semantic similarity\n",
        "\n",
        "In this lesson, you'll learn how to use spaCy to predict how similar documents, spans or tokens are to each other.\n",
        "\n",
        "The `Doc`, `Token` and `Span` objects have a `.similarity` method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
        "\n",
        "\n",
        "**NB!** : In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuViTnC44vr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "# Compare two documents\n",
        "doc1 = nlp(\"I like fast food\")\n",
        "doc2 = nlp(\"I like pizza\")\n",
        "print(doc1.similarity(doc2))\n",
        "\n",
        "# Compare a document with a token\n",
        "doc = nlp(\"I like pizza\")\n",
        "token = nlp(\"soap\")[0]\n",
        "\n",
        "print(doc.similarity(token))\n",
        "\n",
        "# Compare a span with a document\n",
        "span = nlp(\"I like pizza and pasta\")[2:5]\n",
        "doc = nlp(\"McDonalds sells burgers\")\n",
        "\n",
        "print(span.similarity(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk52BqVy6tkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"I have a banana\")\n",
        "# Access the vector via the token.vector attribute\n",
        "print(doc[3].vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYfSfRZs7xus",
        "colab_type": "text"
      },
      "source": [
        "# Combining models and rules\n",
        "\n",
        "Combining statistical models with rule-based systems is one of the most powerful tricks you should have in your NLP toolbox.\n",
        "\n",
        "![alt text](https://imgur.com/mULKyUal.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHSA80hs9IFQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtNIN5kJ9If0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from spacy.lang.en import English\n",
        "\n",
        "with open(\"exercises/en/countries.json\") as f:\n",
        "    COUNTRIES = json.loads(f.read())\n",
        "\n",
        "nlp = English()\n",
        "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
        "\n",
        "# Import the PhraseMatcher and initialize it\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# Create pattern Doc objects and add them to the matcher\n",
        "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
        "patterns = list(nlp.pipe(COUNTRIES))\n",
        "matcher.add(\"COUNTRY\", None, *patterns)\n",
        "\n",
        "# Call the matcher on the test document and print the result\n",
        "matches = matcher(doc)\n",
        "print([doc[start:end] for match_id, start, end in matches])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}